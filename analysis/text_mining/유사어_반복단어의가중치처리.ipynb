{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'nltk' has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-fbcd8416efb7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\analysis\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;31m# Packages which can be lazily imported\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\analysis\\lib\\site-packages\\nltk\\stem\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misri\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mISRIStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnowball\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwordnet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrslp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRSLPStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\analysis\\lib\\site-packages\\nltk\\stem\\snowball.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mporter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msuffix_replace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix_replace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\analysis\\lib\\site-packages\\nltk\\corpus\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyCorpusLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m abc = LazyCorpusLoader(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\analysis\\lib\\site-packages\\nltk\\corpus\\reader\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     54\u001b[0m \"\"\"\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaintext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\analysis\\lib\\site-packages\\nltk\\corpus\\reader\\plaintext.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mPlaintextCorpusReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCorpusReader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \"\"\"\n\u001b[0;32m     25\u001b[0m     \u001b[0mReader\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcorpora\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mconsist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mplaintext\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mParagraphs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\analysis\\lib\\site-packages\\nltk\\corpus\\reader\\plaintext.py\u001b[0m in \u001b[0;36mPlaintextCorpusReader\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m     def __init__(self, root, fileids,\n\u001b[0;32m     41\u001b[0m                  \u001b[0mword_tokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mWordPunctTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m                  sent_tokenizer=nltk.data.LazyLoader(\n\u001b[0m\u001b[0;32m     43\u001b[0m                      'tokenizers/punkt/english.pickle'),\n\u001b[0;32m     44\u001b[0m                  \u001b[0mpara_block_reader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_blankline_block\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'nltk' has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. term frequency\n",
    " - tf\n",
    " - 어떤 단어가 문서 내에서 자주 등장할수록, 중요도가 높아진다 \n",
    "\n",
    "2. inverse document frequency\n",
    " - idf\n",
    " - 비교하는 모든 문서 내에서 만약에 같은 단어가 존재한다면, 핵심 어휘일수 있다\n",
    " - 그러나, 문서간의 비교에서는 중요한 단어가 아닐수있다\n",
    " \n",
    "3. 결론\n",
    " - 텍스트 마이닝에서 사용하는 기법, 단어별에 가중치를 부과하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 구성\n",
    "'''\n",
    "t : 단어 , 'a' or 'b' or 'c'\n",
    "d : 문장을 가진 한세트\n",
    "D : 문장 리스트를 맴버로 가진 리스트(비교할 문장덩어리)\n",
    "'''\n",
    "def tfidf( t, d, D ):\n",
    "    #print( \"float( %s.count(%s) ) / sum( %s.count(w) for w in set(%s) )\" % (d,t,d,d) )\n",
    "    #print( \"%s / %s\" % (float( d.count(t) ), sum( d.count(w) for w in set(d) )) )\n",
    "    tf = float( d.count(t) ) / sum( d.count(w) for w in set(d) )\n",
    "    print( 'sp.log( %s/%s)' %  (float(len(D)), (len([ doc for doc in D if t in doc ]))) )\n",
    "    idf = sp.log( float(len(D)) / (len([ doc for doc in D if t in doc ]))   )\n",
    "    return tf, idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1, 2]\n",
      "[1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# d.count(w) for w in set(d)\n",
    "# 문장안에서 단어별 발생된 총 빈도수의 합산\n",
    "print( [ ['a'].count(w) for w in set(['a']) ] )\n",
    "print( [ ['a','b','b'].count(w) for w in set(['a','b','b']) ] )\n",
    "print( [ ['a','b','c'].count(w) for w in set(['a','b','c']) ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print( sum( [ ['a'].count(w) for w in set(['a']) ] ))\n",
    "print( sum( [ ['a','b','b'].count(w) for w in set(['a','b','b']) ] ))\n",
    "print( sum( [ ['a','b','c'].count(w) for w in set(['a','b','c']) ] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# d.count(t)\n",
    "print( float(['a'].count( 'a' )) )\n",
    "print( float(['a','b','b'].count( 'b' )) )\n",
    "print( float(['a','b','c'].count( 'c' )) )\n",
    "print( float(['a','b','c'].count( 'a' )) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각각 문장\n",
    "a, abb, abc = ['a'],['a','b','b'],['a','b','c']\n",
    "# 여러 문장을 모아 두었다\n",
    "D = [ a, abb, abc ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sp.log( 3.0/3)\n",
      "(1.0, 0.0)\n",
      "sp.log( 3.0/2)\n",
      "(0.6666666666666666, 0.4054651081081644)\n",
      "sp.log( 3.0/3)\n",
      "(0.3333333333333333, 0.0)\n",
      "sp.log( 3.0/2)\n",
      "(0.3333333333333333, 0.4054651081081644)\n",
      "sp.log( 3.0/1)\n",
      "(0.3333333333333333, 1.0986122886681098)\n"
     ]
    }
   ],
   "source": [
    "# 문장 전체의 단어 빈도수 대비 해당 단어의 빈도수의 비율 : tf\n",
    "# 단어 빈도가 1인데 a라는 단어도 1이므로   tf = 1\n",
    "print( tfidf( 'a', a, D ) )\n",
    "# 단어 빈도수가 3인데  b 는 2번 등장했으므로 tf=0.67\n",
    "print( tfidf( 'b', abb, D ) )\n",
    "# 단어 빈도수가 3인데  b 는 1번 등장했으므로 tf=0.33\n",
    "print( tfidf( 'a', abc, D ) )\n",
    "print( tfidf( 'b', abc, D ) )\n",
    "print( tfidf( 'c', abc, D ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리로 처리\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer( min_df=1, decode_error='ignore' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석기\n",
    "from konlpy.tag import Okt\n",
    "t = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = ['성건이랑 술마시고 싶지만 바쁜데 어떻하죠?',\n",
    "            '성건이는 공원에서 산책하고 노는 것을 싫어해요',\n",
    "            '성건이는 공원에서 노는 것도 싫어해요. 이상해요.',\n",
    "            '먼 곳으로 여행을 떠나고 싶은데 너무 바빠서 그러질 못하고 있어요.'\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['성', '건', '이랑', '술', '마시고', '싶지만', '바쁜데', '어떻하죠', '?'],\n",
       " ['성', '건', '이', '는', '공원', '에서', '산책', '하고', '노', '는', '것', '을', '싫어해요'],\n",
       " ['성',\n",
       "  '건',\n",
       "  '이',\n",
       "  '는',\n",
       "  '공원',\n",
       "  '에서',\n",
       "  '노',\n",
       "  '는',\n",
       "  '것',\n",
       "  '도',\n",
       "  '싫어해요',\n",
       "  '.',\n",
       "  '이상해요',\n",
       "  '.'],\n",
       " ['먼',\n",
       "  '곳',\n",
       "  '으로',\n",
       "  '여행',\n",
       "  '을',\n",
       "  '떠나고',\n",
       "  '싶은데',\n",
       "  '너무',\n",
       "  '바빠서',\n",
       "  '그러질',\n",
       "  '못',\n",
       "  '하고',\n",
       "  '있어요',\n",
       "  '.']]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents_tokens = [ t.morphs( doc ) for doc in contents ]\n",
    "contents_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contents_tokens), len(contents_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 기반으로 문장을 재구성\n",
    "# => \"품질 이랑 술 마시고 싶지만 바쁜데 어떻하죠 ?\"\n",
    "# contents_for_vectorize 여기에 재구성한 문장을 담는다\n",
    "contents_for_vectorize = [ ' '.join(content) for content in contents_tokens ]\n",
    "\n",
    "# for content in contents_tokens:    \n",
    "#     tmp = ''\n",
    "#     print( ' '.join(content) )\n",
    "#     for word in content:\n",
    "#         tmp = tmp + ' ' + word\n",
    "        \n",
    "#     contents_for_vectorize.append( tmp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['성 건 이랑 술 마시고 싶지만 바쁜데 어떻하죠 ?',\n",
       " '성 건 이 는 공원 에서 산책 하고 노 는 것 을 싫어해요',\n",
       " '성 건 이 는 공원 에서 노 는 것 도 싫어해요 . 이상해요 .',\n",
       " '먼 곳 으로 여행 을 떠나고 싶은데 너무 바빠서 그러질 못 하고 있어요 .']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents_for_vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 19)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 백터화 변환\n",
    "X = vectorizer.fit_transform( contents_for_vectorize )\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 19)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터의 크기를 변수로 받는다 \n",
    "num_samples, num_features = X.shape\n",
    "num_samples, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['공원', '그러질', '너무']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 백터의 특성화값 -> 컬럼-> 변수, 특성\n",
    "vectorizer.get_feature_names()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['근처', '공원', '에서', '성', '건', '이랑', '놀러', '가고', '싶어요']]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 신규 문장 -> 백터화\n",
    "# 1. 신문장\n",
    "#new_post = ['성건이랑 공원에서 산책하고 놀고 싶어요']\n",
    "new_post = ['근처 공원에서 성건이랑 놀러가고 싶어요']\n",
    "# 2. 형태소 분리\n",
    "new_post_tokens = [ t.morphs( doc ) for doc in new_post ]\n",
    "new_post_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['근처 공원 에서 성 건 이랑 놀러 가고 싶어요']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 형태소 단위로 문장 재구성\n",
    "new_post_for_vectorize = [ ' '.join(content) for content in new_post_tokens ]\n",
    "new_post_for_vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 19)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. 백터화\n",
    "new_post_vec = vectorizer.transform( new_post_for_vectorize )\n",
    "new_post_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 거리 계산(백터간)=> 정규화를 적용하여\n",
    "def dist_norm( v1, v2 ):\n",
    "    # 각각 백터 문장의 정규화\n",
    "    v1_norm = v1 / sp.linalg.norm( v1.toarray() )\n",
    "    v2_norm = v2 / sp.linalg.norm( v2.toarray() )\n",
    "    # 두 문장의 거리 (차이) 계산\n",
    "    delta = v1_norm - v2_norm\n",
    "    # 결과 리턴\n",
    "    return sp.linalg.norm( delta.toarray() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1844028684759158\n",
      "1.0539963851720047\n",
      "1.009546132854007\n",
      "1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "# 유사도 검사\n",
    "best_doc      = None\n",
    "best_distance = 1000\n",
    "best_index    = None\n",
    "for i in range( num_samples ):    \n",
    "    # 비교 문장의 백터값 획득\n",
    "    post_vec = X.getrow(i)   \n",
    "    # 거리 계산\n",
    "    d        = dist_norm( post_vec,  new_post_vec )\n",
    "    print( d )\n",
    "    # 기준값보다 작을때 처리\n",
    "    if d < best_distance:\n",
    "        # 거리값\n",
    "        best_distance = d\n",
    "        # 유사한 인덱스\n",
    "        best_index    = i\n",
    "        # 유사한 문장\n",
    "        best_doc      = contents[ best_index ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.009546132854007"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결과 확인\n",
    "best_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'성건이는 공원에서 노는 것도 싫어해요. 이상해요.'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['성건이랑 술마시고 싶지만 바쁜데 어떻하죠?',\n",
       " '성건이는 공원에서 산책하고 노는 것을 싫어해요',\n",
       " '성건이는 공원에서 노는 것도 싫어해요. 이상해요.',\n",
       " '먼 곳으로 여행을 떠나고 싶은데 너무 바빠서 그러질 못하고 있어요.']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['근처 공원에서 성건이랑 놀러가고 싶어요']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
