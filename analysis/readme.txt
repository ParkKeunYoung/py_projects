1. 아나콘다 가상환경 구축
   - Aanconda nevigator 가동
   - Environments > create > Datarawling > python 3.6 > 생성
   - cd ~/analysis/crawling/requirements.txt 생성 
   - 해당 프로젝트에서 사용하는 패키지를 서술
     ---------------------------------------
     requests==2.18.4
     beautifulsoup4==4.6.0
     pandas==0.23.0
     numpy==1.14.3
     ---------------------------------------
   - Environments >Create > DataCrawling > open the terminal
       $ cd 프로젝트 위치(requirements.txt 위치)
     가상환경내에 설치된 패키지 목록 보기
       (DataCrawling)$ conda list 혹은 pip list (단, 약간의 목록 차이는 존재한다.)
     프로젝트에 필요한 패키지 설치
       (DataCrawling)$ pip install -r requirements.txt
       or
       (DataCrawling)$ conda install --file requirements.txt
2. 주피터 구동

3. 데이터
   - 현재상황
   1) 빅데이터
      원유, 전통적인 데이터 관리법으로 다루기 힘든 데이터를 다 아우른다.
   
   2) 데이터 과학 -> 새로운 가치(value)를 생성
      정유공장 -> 대량의 데이터를 분석하여, 지식을 추출하는 방법까지 아우른다.
   
   3) 구분
      > 빅테이터와 전통적인 데이터와 구분
      - 규모   : 얼마나 많은 데이터가 존재하고, 계속 생성되는지
      - 다양성 : 데이터의 종류가 얼마나 많은가
      - 속도   : 새로운 데이터가 얼마나 많이 빠르게 생성되는가
      - 정확성 : 데이터가 얼마나 정확한가
   
   4) 통계학자, 데이터 과학자 주된 차이점
      - 빅데이터를 다루는 능력, 하둡(스파크, 클러스터) 참고
      - 머신러닝(딥러닝) : AI > 머신러닝 > 딥러닝
      - 컴퓨팅능력
      - 알고리즘에 대한 구축 능력(새로만드는가? 조합을 만들어 내는가?)
   
   5) 활용
      - 산업 : 고객, 업무, 직원, 상품에 대한 "통찰"을 얻기위해 데이터 과학과 빅데이터를 활용
      - 기업 : 교차판매, 상향 판매, 개인화등등 -> 구글 에드샌드 활용 -> 맞춤광고 등 
      - 예   : 
               a) 피플 애널리스트 => 텍스트 마이닝 적용 => 직원사기점검, 직원 간 네트워크 연구 => 머니볼( 메이저리그 특정 및 이야기 , 데이터 분석을
                  통해 발전시킨사례)
               b) 금융기관
                  => 주식시장예측, 대출 위험 평가, 신규 고객유치, 퀀트들의 개발 알고리즘을 통한 컴퓨터 자동 거래
               c) 정부기관
                  => 공공 데이터를 공개, 이를 통한 데이터 통찰을 수행할 수 있는 기회 제공, 어플의 개발유도
                  => 수백만의 개인 감시용으로 사용: 빅브라더 => 이메일, 지도, 검색, 게임 등등 통해 수백만의 데이터를 수집하고 예측
               d) 비정부기관
                  => NGO -> 자원봉사를 통해서 데이터를 구축 및 분석
               e) 대학
                  => 연구, 학습경험 개선, 코세라, 유닷시티, 에덱스, 
                  => 다양한 대학 기관들이 데이터를 제공, 코렐, 머신러닝 때 따로 제공
 
   - 데이터의 종류
       1) 구조적 데이터
          - 데이터 모델에 의존적인 구조
          - 레코드 내에 고정된 필드가 존재
          - 데이터베이서의 테이블, 엑셀파일
          - 관련조회 => sql(구조화된 질의 언어), 비구조적 데이터에 비해 사이즈가 작다.
          - 정제된 데이터
       
       2) 비구조적 데이터
          - 데이터 모델에 적합하지 않는 데이터, 잘 안맞는다. 
          - 이메일 (발신자, 제목, 본문 구조적이나, 본문의 내용은 비구조적이다, 정의할 수 없다.)
       
       3) 자연어 데이터
          - 언어학에 대한 지식을 필요로하고, 처리가 까다롭다.
          - 이미 파악된 분야 : 개체인식, 주제 파악, 요약문 작성, 텍스트 완성, 정서(감정) 분석 등
          - 문제점 : 한분야에 맞는 모델을 개발하면 다른 분야로 일반화를 할 수 없다.
          - 한계 : 텍스트의 행간이라는 분야는 아직 풀어내지 못하는 단계, 같은 단어도 감정이 다르게 표출 되기 때문에 어려운 분야 
       
       4) 기계 생성 데이터
          - 사람의 개입없이 자동으로 생성된 정보
          - 주체 : 컴퓨터, 프로세서, 어플리케이션, 장비, 센서
          - 해당 데이터는 주요한 데이터 공급원
          - 산업데이터의 시장 가치 예측은 2020년 기준 600조원 예측(위키본)
          - IDC 쪽에서는 사물간 연결에서 발생되는 데이터는 기존의 26배로 예측
       
       5) 그래프 기반 데이터( 네트워크 데이터 ) -> 소셜네트워크
          - 수학 장르의 그래프 이론을 기반으로 데이터 설명
          - 그래프 : 짝을 이루는 객체 간의 관계를 모델링하는 수학적인 구조
            -> 객체 간의 관계, 인접에 촛점을 맞춘 데이터이다.
          - 구성 : 꼭지점(node), 변(edge), 속성(property)들을 가지고 데이터를 표현
          - 예시
            ·소셜 네트워크를 표현하는데 가장 적합, 어떤 사람의 영향력을 평가, 사람들 사이의 최단 경로의 수치화
              ex) 링크드인 : 누가 어떤 회사에 다니는가 
                  트위터, 페이스북, 인스타그램 : 팔로워 
                  페이스북의 친구, 링크드인상의 친구> 접점, 넷플릭스에서 영화 취향
                  => 새로운 데이터 통찰 -> 질의 -> 데이터를 분석, 규명, 설명

       6) 오디오,비디오 데이터
          - 가장 까다로운 데이터
          - 2014년 기준 MLB에서 생중계 시, 경기 분석을 하기위해서 영상의 분량을 경기단 7TB로 늘려서 처리.
            고속 카메라가 공, 선수의 움직임을 다 표현하여 베이스라인과 비교 등을 실시간 계산 처리
          - 딥마인드 : 비디오게임 플레이 알고리즘 개발 -> 구글 인수 -> 알파고
       
       7) 스트리밍 데이터
          - 어떤 사건이 발생 -> 데이터가 발생 -> 시스템으로 흘러들어간다.
          - 트위터의 실시간 트랜드, 운동경기 생중계 데이터, 공연 데이터, 주식시장 데이터 
       
   - 연구목표 설정
     1) 통계/시각적 분석 => 이미 축적된 과거의 데이터를 기반으로 새로운 정보 -> 지식 -> 지혜까지 추출하는 과정
        - 그래프데이터를 이용한 요리 추천 분석
        - 부산시 CCTV 현황분석
        - 미래에 우리나라의 어느 지역이 인구나 몇 년 내에 소멸하여, 이에 따른 정책 변환 추진(붙이기나름)
        - 대선, 총선 결과 분석
        - 주식데이터(시계열) 분석을 통한 근거리 주식장 분석
        - 스타트업 기업 통계 분석
        - 음악 데이터를 이용한 년도별 인기 통계를 기반으로 향후 유행장르 예측
        - 항공 운항데이터나 고객 데이터를 이용한 마케팅 분석
          => 특가!! => 분석을 통한 고객 유입
          => 수화물을 무료로 붙이면서 싸게 >> 진에어, 
     
     2) 머신러닝/지도학습 => 학습, 예측
                          => 이미 변수(특성)들과 답이 존재한다. => 이미 학습을 시키고 한 번도 접해보지 못한 데이터를 예측 했을 때 그 정확도
        - 편지 봉투에 손글씨로 작성한 우편번호 숫자를 판별
        - 의료 영상 이미지 기반 종양/암 등 질병 판단 (영상분석)
        - 의심스러운 신용카드 거래 감지 >> 이상 징후 감지/탐지
        - 텍스트 마이닝 기반으로 게시글 등을 텍스트 전체 맥락 분석 
        - 영화 평점, 리뷰 등을 기반으로 넷플릭스 영화 추천 시스템의 정확도 1% 향상
        - 업리프트 모델링으로 마케팅의 효율성을 증가
        - ( 역학통계의 다이렉트 마케팅기법 => 무작위 대조시험의 결과를 분석하여 신약이 효과가 있었는가?, 
            광고 메일을 어떤 유형읙 고객에게 보내야 효과가 있는가? )
        - 알파벳의 언어 빈도 차이를 기반으로 언어를 판독해 내는 기술
        - 게임 접속 로그 및 데이터를 제공하여 => 고객 이탈을 예측하는 모델
        - 연비 예측
        - 콘크리트 혼합물의 압축 강도 예측
        - 스마트카 배기량을 분석에 따른 운전자 연소득 예측
        
     3) 머신러닝/비지도학습 => 답이 없다
        - 블러그 글의 주제 구분해 내는 모델
        - 고객 취향 유형에 맞춰서 그룹 분류
        - 비정상적인 웹사이트 탐지
        
     4) 딥러닝
        - 자율 주행을 위한 영상 분석
        - RNN을 이용한 번역 서비스
        - RNN을 이용한 챗봇
        - CNN과 opencv를 이용한 영상 인식
        - 자율주행중인 스마트카의 위험 징후 판별
        - 사운드, 음성분석
     
   - 데이터과학 진행과정

     1) 연구목표 설정
        무엇을 조사할것인지. 그 분석 결과로 회사/공공에서는 어떤 이익을 도출할 것인지.
        어떤 데이터와 자원이 필요한가? 일정, 업무분장은 어떻게?
        가장 중요한 것은 최종 결과물에 대한 검토!! <- 시작점
        이런 결과물은 의사 결정에 재료가 될수도, 업무의 한 파트가 될 수도 있다.
     2) 데이터 획득
        2-1. level 1
             - 제공이 된다.
             - 사내 데이터, 공공 데이터, 대학 및 연구기관의 제공 데이터
             - 콘테스트 데이터(국내대회, 해외대회(캐글 kaggle))        
             ==> 상업성이 없고, 대부분 정제된 데이터다.
        2-2. level 2
             - open AIP 사용 
             - http 통신을 통해서 응답 데이터를 통해 수집
             - ex) kakao, naver, t, google 등등 포털이나 대기업이 제공하는 open API를 활용
             - 단, 쿼리 제한(일일 쿼리수)
             - 정제된 데이터다.
             - request
        2-3. level 3
             - web scraping(웹 스크래핑)
             - 우리가 접근할 수 있는 모든 정보는 웹에서 접근이 가능하다.라는 명제로 출발
             - 보안 데이터는 불가
             - 웹사이트를 긁어서 원하는 데이터를 추출하여 전처리 적제하는 방식
             - request + beautifulsoup(bs4)
        2-4. level 4
             - crawling(크롤링)
             - 정보의 출처가 웹사이트는 맞는데 사람의 손을 타야 데이터를 획들할 수 있는 경우
             - ajax를 사용하거나 디도스 방어가 들어갔거나, 등등 사람손을 거친 후에야야만 접근 가능한 사이트가 대상
             - selenium(셀레니움) + 자동화 ( qt5 or 스케쥴러를 활용)
              
     3) 데이터 준비
             - pandas(데이터처리, 분석), numpy(수학, 과학용) 등을 사용
             - 데이터의 품질을 향상시킨다. 여기에 주안점
        3-1 데이터 정제 : 결측치, 이상치 처리
        3-2 데이터 통합 : 여러 군데서 가져온 데이터를 조합, 데이터 구성
        3-3 데이터 변환 : 데이터를 모델(4~5단계)에서 적합하게 사용되도록 변경 처리

     4) 데이터 탐색
             - pandas, matplotlib(시각화), seabon, d3.js
             - 데이터 깊이를 이해하는데 중점 -> 통찰 
             - 변수들의 상호 작용성, 시각적 분석법, 이상점 존재여부 체크 
             - 통계적 분석, 시각적 분석, 단순 모델링 등을 사용

     5) 데이터 모델링 및 모델 구축
             - scikit-learn(머신러닝), tensorflow(딥러닝), keras(딥러닝)
             - 이전 단계로 부터 획득한 모델, 도메인 지식, 데이터에 대한 통찰 등을 이용하여
               연구 목표에 대한 답을 찾는 과정.
               머신러닝, 딥러닝 : 예측모델을 구성, 정확도 높이고, 평가 지수 고려 등등 과정진행
               => 목표치에 도달하지 못하면 다시 원점으로 돌아가서 다시 시작한다. ( 전면 제검토 )
             - 필요하다면 통계학도 사용, 머신러닝, 딥러닝 등등 운영과학기법들을 총동원
             - 머신러닝[ 학습, 정확도평가, 파이프라인, 하이퍼파라미터 튜닝, 성능평가 ]

     6) 시스템통합 혹은 레포트 발표
             - 1단계에서 정한 결론에 대한 마무리
             - 레포트 형태, 보고서 형태, 시스템 형태, 솔루션 형태 등 다양한 모습으로 결론이 도출된다.
             - 시스템 형태(웹기반: flask, DJango // GUI기반: gt5// 백그라운드서비스: 순수python+OS종속적구조)

   - 데이터 획득
   
5. 시각화의 카테고리
   - 시간 시각화
     > 막대(바)차트, 누적막대, 점그래프(산포도), 선그래프(연속데이터)
   - 분포 시각화
     > 파이차트(원), 도넛차트, 트리맵
   - 관계 시각화
     > 산점도, 산포도, 버블차트, 히스토그램, 네트워크 그래프(networkX)
   - 비교 시각화
     > 히트맵, 체프론 페이스, 스타차트, 평행좌표계
   - 공간 시각화
     > 지도,GIS