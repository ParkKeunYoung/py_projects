1. 아나콘다 가상환경 구축
   - Aanconda nevigator 가동
   - Environments > create > Datarawling > python 3.6 > 생성
   - cd ~/analysis/crawling/requirements.txt 생성 
   - 해당 프로젝트에서 사용하는 패키지를 서술
     ---------------------------------------
     requests==2.18.4
     beautifulsoup4==4.6.0
     pandas==0.23.0
     numpy==1.14.3
     ---------------------------------------
   - Environments >Create > DataCrawling > open the terminal
       $ cd 프로젝트 위치(requirements.txt 위치)
     가상환경내에 설치된 패키지 목록 보기
       (DataCrawling)$ conda list 혹은 pip list (단, 약간의 목록 차이는 존재한다.)
     프로젝트에 필요한 패키지 설치
       (DataCrawling)$ pip install -r requirements.txt
       or
       (DataCrawling)$ conda install --file requirements.txt
2. 주피터 구동

3. 데이터
   - 현재상황

   - 데이터의 종류

   - 데이터과학 진행과정

     1) 연구목표 설정
        무엇을 조사할것인지. 그 분석 결과로 회사/공공에서는 어떤 이익을 도출할 것인지.
        어떤 데이터와 자원이 필요한가? 일정, 업무분장은 어떻게?
        가장 중요한 것은 최종 결과물에 대한 검토!! <- 시작점
        이런 결과물은 의사 결정에 재료가 될수도, 업무의 한 파트가 될 수도 있다.
     2) 데이터 획득
        2-1. level 1
             - 제공이 된다.
             - 사내 데이터, 공공 데이터, 대학 및 연구기관의 제공 데이터
             - 콘테스트 데이터(국내대회, 해외대회(캐글 kaggle))        
             ==> 상업성이 없고, 대부분 정제된 데이터다.
        2-2. level 2
             - open AIP 사용 
             - http 통신을 통해서 응답 데이터를 통해 수집
             - ex) kakao, naver, t, google 등등 포털이나 대기업이 제공하는 open API를 활용
             - 단, 쿼리 제한(일일 쿼리수)
             - 정제된 데이터다.
             - request
        2-3. level 3
             - web scraping(웹 스크래핑)
             - 우리가 접근할 수 있는 모든 정보는 웹에서 접근이 가능하다.라는 명제로 출발
             - 보안 데이터는 불가
             - 웹사이트를 긁어서 원하는 데이터를 추출하여 전처리 적제하는 방식
             - request + beautifulsoup(bs4)
        2-4. level 4
             - crawling(크롤링)
             - 정보의 출처가 웹사이트는 맞는데 사람의 손을 타야 데이터를 획들할 수 있는 경우
             - ajax를 사용하거나 디도스 방어가 들어갔거나, 등등 사람손을 거친 후에야야만 접근 가능한 사이트가 대상
             - selenium(셀레니움) + 자동화 ( qt5 or 스케쥴러를 활용)
              
     3) 데이터 준비
             - pandas(데이터처리, 분석), numpy(수학, 과학용) 등을 사용
             - 데이터의 품질을 향상시킨다. 여기에 주안점
        3-1 데이터 정제 : 결측치, 이상치 처리
        3-2 데이터 통합 : 여러 군데서 가져온 데이터를 조합, 데이터 구성
        3-3 데이터 변환 : 데이터를 모델(4~5단계)에서 적합하게 사용되도록 변경 처리

     4) 데이터 탐색
             - pandas, matplotlib(시각화), seabon, d3.js
             - 데이터 깊이를 이해하는데 중점 -> 통찰 
             - 변수들의 상호 작용성, 시각적 분석법, 이상점 존재여부 체크 
             - 통계적 분석, 시각적 분석, 단순 모델링 등을 사용

     5) 데이터 모델링 및 모델 구축
             - scikit-learn(머신러닝), tensorflow(딥러닝), keras(딥러닝)
             - 이전 단계로 부터 획득한 모델, 도메인 지식, 데이터에 대한 통찰 등을 이용하여
               연구 목표에 대한 답을 찾는 과정.
               머신러닝, 딥러닝 : 예측모델을 구성, 정확도 높이고, 평가 지수 고려 등등 과정진행
               => 목표치에 도달하지 못하면 다시 원점으로 돌아가서 다시 시작한다. ( 전면 제검토 )
             - 필요하다면 통계학도 사용, 머신러닝, 딥러닝 등등 운영과학기법들을 총동원
             - 머신러닝[ 학습, 정확도평가, 파이프라인, 하이퍼파라미터 튜닝, 성능평가 ]

     6) 시스템통합 혹은 레포트 발표
             - 1단계에서 정한 결론에 대한 마무리
             - 레포트 형태, 보고서 형태, 시스템 형태, 솔루션 형태 등 다양한 모습으로 결론이 도출된다.
             - 시스템 형태(웹기반: flask, Django // GUI기반: qt5// 백그라운드서비스: 순수python+OS종속적구조)

   - 데이터 획득